{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория вероятностей и математическая статистика\n",
    "\n",
    "## Урок 1\n",
    "\n",
    "## Случайные события. Вероятность события. Условная вероятность. Формула Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Наивный Байесовский классификатор__\n",
    "\n",
    "Формула Байеса является основой для одной из моделей машинного обучения, решающих задачу классификации — __Наивного Байесовского классификатора__ (__Naive Bayes classifier__). \n",
    "\n",
    "Допустим, имеется набор объектов $X = (x_1, x_2, \\dots, x_m)$ с признаками $x_1$, $\\dots$, $x_m$, причём каждому объекту $X_i$ приписана метка класса $c_i$. Байесовский классификатор основан на зависимости между двумя вероятностями:\n",
    "1. $P(c \\mid x_1, \\dots, x_m)$, т.е. вероятность, что объект с признаками $x_1$, $\\dots$, $x_m$ будет иметь метку класса $c$,\n",
    "2. $P(x_1, \\dots, x_m \\mid c)$ — вероятность того, объект, взятый случайным образом из класса $C$ будет обладать признаками $x_1$, $\\dots$, $x_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взаимосвязь между этими вероятностями определяется формулой Байеса:\n",
    "$$P(c \\mid x_1, \\dots, x_m) = \\dfrac{P(x_1, \\dots, x_m \\mid c) \\cdot P(c)}{P(x_1, \\dots, x_m)}$$\n",
    "\n",
    "Собственно, Байесовский классификатор определяется следующим образом:\n",
    "$$C(X) = \\arg\\max_c P(c \\mid X),$$\n",
    "т.е. объект $X$ причисляем к тому классу $c$, для которого указанная выше вероятность максимальна.\n",
    "\n",
    "Почему классификатор называется __наивным__? В указанной выше формуле нет никакой наивности. Однако, оценка вероятности $P(c \\mid x_1, \\dots, x_m)$ без дополнительных условий является крайне сложной задачей. Для того, чтобы упростить эту оценку, на рассматриваемые признаки накладывается условие __независимости__. (Это условие и является крайне наивным, поскольку в общем случае ожидать независимости признаков нет никаких оснований.) Как мы установили выше, условие независимости даёт нам возможность разбивать вероятности вида $P(x_1, \\dots, x_m \\mid c)$ в произведения $P(x_1 \\mid c) \\cdots P(x_m \\mid c)$, что изрядно упрощает вычисление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, используя независимость признаков, __итоговую модель__ можно записать в следующем виде:\n",
    "$$C(X) = \\arg\\max_c \\left( P(c) \\cdot \\displaystyle\\prod_X P(x_1 \\mid c) \\cdots P(x_m \\mid c) \\right)$$\n",
    "\n",
    "Отметим, что в этой формуле не хватает выражения $P(x_1, \\dots, x_m)$ из знаменателя. От этого выражения часто избавляются, поскольку оно никак не зависит от выбора конкретного класса $c$, поэтому вносит одинаковый вклад в каждую из вероятностей $P(c \\mid X)$.\n",
    "\n",
    "Для обучения вероятностей $P(x_i \\mid c)$, как правило, используют дополнительные предположения о распределении отдельных признаков, наиболее популярна модель Гауссов Байесовский классификатор (Gaussian Bayes classifier), предполагающая нормальное распределение признаков. О нормальном распределении мы поговорим на уроке 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
